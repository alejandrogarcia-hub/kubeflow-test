# PIPELINE DEFINITION
# Name: iris-classification-pipeline
# Description: End-to-end ML pipeline with training, evaluation, serving, and monitoring
# Inputs:
#    accuracy_threshold: float [Default: 0.85]
#    f1_threshold: float [Default: 0.85]
#    model_name: str [Default: 'iris-classifier']
#    model_version: str [Default: 'v1.0.0']
#    n_estimators: int [Default: 100.0]
#    random_state: int [Default: 42.0]
#    test_size: float [Default: 0.2]
components:
  comp-condition-1:
    dag:
      tasks:
        prepare-model-serving:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-prepare-model-serving
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-iris-model-model
              scaler:
                componentInputArtifact: pipelinechannel--train-iris-model-scaler
            parameters:
              model_dir:
                componentInputParameter: pipelinechannel--train-iris-model-Output
          taskInfo:
            name: Prepare Model Serving
        register-model:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-register-model
          inputs:
            artifacts:
              evaluation_report:
                componentInputArtifact: pipelinechannel--evaluate-model-evaluation_report
              model:
                componentInputArtifact: pipelinechannel--train-iris-model-model
            parameters:
              model_name:
                componentInputParameter: pipelinechannel--model_name
              model_version:
                componentInputParameter: pipelinechannel--model_version
          taskInfo:
            name: Register Model
        setup-drift-monitoring:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-setup-drift-monitoring
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-iris-model-model
          taskInfo:
            name: Setup Drift Monitoring
    inputDefinitions:
      artifacts:
        pipelinechannel--evaluate-model-evaluation_report:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        pipelinechannel--train-iris-model-model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        pipelinechannel--train-iris-model-scaler:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--evaluate-model-Output:
          parameterType: STRING
        pipelinechannel--model_name:
          parameterType: STRING
        pipelinechannel--model_version:
          parameterType: STRING
        pipelinechannel--train-iris-model-Output:
          parameterType: STRING
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        accuracy_threshold:
          parameterType: NUMBER_DOUBLE
        f1_threshold:
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        evaluation_report:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
      parameters:
        Output:
          parameterType: STRING
  comp-prepare-model-serving:
    executorLabel: exec-prepare-model-serving
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        model_dir:
          parameterType: STRING
    outputDefinitions:
      parameters:
        serving_uri:
          parameterType: STRING
  comp-register-model:
    executorLabel: exec-register-model
    inputDefinitions:
      artifacts:
        evaluation_report:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_name:
          parameterType: STRING
        model_version:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        registry_entry:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-setup-drift-monitoring:
    executorLabel: exec-setup-drift-monitoring
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        monitoring_config:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-train-iris-model:
    executorLabel: exec-train-iris-model
    inputDefinitions:
      parameters:
        n_estimators:
          parameterType: NUMBER_INTEGER
        random_state:
          parameterType: NUMBER_INTEGER
        test_size:
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        Output:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==1.5.2'\
          \ 'pandas==2.2.3' 'numpy==1.26.4'  &&  python3 -m pip install --quiet --no-warn-script-location\
          \ 'kfp==2.14.1' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"\
          3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(\n    model: Input[Model],\n    scaler: Input[Artifact],\n\
          \    accuracy_threshold: float,\n    f1_threshold: float,\n    evaluation_report:\
          \ Output[Artifact],\n    metrics: Output[Metrics],\n) -> str:\n    \"\"\"\
          \n    Evaluate trained model and make deployment decision.\n\n    Returns:\n\
          \        str: \"deploy\" or \"no-deploy\" based on thresholds\n    \"\"\"\
          \n    import json\n    import pickle\n\n    import pandas as pd\n    from\
          \ sklearn.datasets import load_iris\n    from sklearn.metrics import accuracy_score,\
          \ classification_report, f1_score\n    from sklearn.model_selection import\
          \ train_test_split\n\n    print(\"Evaluating model performance...\")\n\n\
          \    # Load model and scaler\n    with open(model.path, \"rb\") as f:\n\
          \        rf_model = pickle.load(f)\n\n    with open(scaler.path, \"rb\"\
          ) as f:\n        scaler_obj = pickle.load(f)\n\n    # Load test data (same\
          \ split as training)\n    iris = load_iris()\n    X = pd.DataFrame(iris.data,\
          \ columns=iris.feature_names)\n    y = pd.Series(iris.target)\n\n    _,\
          \ X_test, _, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42,\
          \ stratify=y\n    )\n\n    # Scale and predict\n    X_test_scaled = scaler_obj.transform(X_test)\n\
          \    y_pred = rf_model.predict(X_test_scaled)\n\n    # Calculate metrics\n\
          \    accuracy = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test,\
          \ y_pred, average=\"weighted\")\n\n    # Log metrics\n    metrics.log_metric(\"\
          evaluation_accuracy\", accuracy)\n    metrics.log_metric(\"evaluation_f1_score\"\
          , f1)\n    metrics.log_metric(\"accuracy_threshold\", accuracy_threshold)\n\
          \    metrics.log_metric(\"f1_threshold\", f1_threshold)\n\n    # Make deployment\
          \ decision\n    deploy = accuracy >= accuracy_threshold and f1 >= f1_threshold\n\
          \    deploy_decision = \"deploy\" if deploy else \"no-deploy\"\n\n    metrics.log_metric(\"\
          deployment_decision\", 1 if deploy else 0)\n\n    # Generate evaluation\
          \ report\n    report = {\n        \"accuracy\": accuracy,\n        \"f1_score\"\
          : f1,\n        \"accuracy_threshold\": accuracy_threshold,\n        \"f1_threshold\"\
          : f1_threshold,\n        \"deploy_decision\": deploy_decision,\n       \
          \ \"classification_report\": classification_report(\n            y_test,\
          \ y_pred, target_names=iris.target_names.tolist(), output_dict=True\n  \
          \      ),\n    }\n\n    # Save report\n    with open(evaluation_report.path,\
          \ \"w\") as f:\n        json.dump(report, f, indent=2)\n\n    print(f\"\
          Evaluation complete - Deploy: {deploy_decision}\")\n    return deploy_decision\n\
          \n"
        image: python:3.11-slim
    exec-prepare-model-serving:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - prepare_model_serving
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'minio==7.2.10'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef prepare_model_serving(\n    model: Input[Model],\n    scaler:\
          \ Input[Artifact],\n    model_dir: str,\n    serving_uri: OutputPath(str),\n\
          ) -> None:\n    \"\"\"\n    Prepare model for serving with KServe.\n\n \
          \   This component packages the model and uploads to storage.\n    \"\"\"\
          \n    import os\n    import shutil\n    from pathlib import Path\n\n   \
          \ print(\"Preparing model for serving...\")\n\n    # Create serving directory\n\
          \    serving_path = Path(\"/tmp/model_serving\")\n    serving_path.mkdir(parents=True,\
          \ exist_ok=True)\n\n    # Copy model artifacts\n    shutil.copy(model.path,\
          \ serving_path / \"model.pkl\")\n    shutil.copy(scaler.path, serving_path\
          \ / \"scaler.pkl\")\n\n    # Copy model info\n    if os.path.exists(f\"\
          {model_dir}/model_info.json\"):\n        shutil.copy(f\"{model_dir}/model_info.json\"\
          , serving_path / \"model_info.json\")\n\n    # In production, upload to\
          \ S3/GCS/MinIO\n    # For now, just save the local path\n    with open(serving_uri,\
          \ \"w\") as f:\n        f.write(str(serving_path))\n\n    print(f\"Model\
          \ prepared for serving at: {serving_path}\")\n\n"
        image: python:3.11-slim
    exec-register-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - register_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pydantic==2.11.0'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef register_model(\n    model: Input[Model],\n    evaluation_report:\
          \ Input[Artifact],\n    model_name: str,\n    model_version: str,\n    registry_entry:\
          \ Output[Artifact],\n) -> None:\n    \"\"\"\n    Register model in the model\
          \ registry.\n\n    Creates model registry entry with metadata.\n    \"\"\
          \"\n    import json\n    from datetime import datetime\n\n    print(f\"\
          Registering model: {model_name} v{model_version}\")\n\n    # Load evaluation\
          \ report\n    with open(evaluation_report.path) as f:\n        eval_report\
          \ = json.load(f)\n\n    # Create registry entry\n    registry_metadata =\
          \ {\n        \"model_name\": model_name,\n        \"model_version\": model_version,\n\
          \        \"model_path\": model.path,\n        \"registered_at\": datetime.now().isoformat(),\n\
          \        \"framework\": \"scikit-learn\",\n        \"algorithm\": \"RandomForestClassifier\"\
          ,\n        \"metrics\": {\n            \"accuracy\": eval_report[\"accuracy\"\
          ],\n            \"f1_score\": eval_report[\"f1_score\"],\n        },\n \
          \       \"status\": \"production-ready\",\n        \"tags\": [\"iris\",\
          \ \"classification\", \"ml-pipeline\"],\n    }\n\n    # Save registry entry\n\
          \    with open(registry_entry.path, \"w\") as f:\n        json.dump(registry_metadata,\
          \ f, indent=2)\n\n    print(\"Model registered successfully\")\n\n"
        image: python:3.11-slim
    exec-setup-drift-monitoring:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - setup_drift_monitoring
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'evidently==0.4.40'\
          \ 'pandas==2.2.3' 'scikit-learn==1.5.2'  &&  python3 -m pip install --quiet\
          \ --no-warn-script-location 'kfp==2.14.1' '--no-deps' 'typing-extensions>=3.7.4,<5;\
          \ python_version<\"3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef setup_drift_monitoring(\n    model: Input[Model], monitoring_config:\
          \ Output[Artifact]\n) -> None:\n    \"\"\"\n    Setup drift monitoring configuration.\n\
          \n    Creates baseline data and monitoring configuration.\n    \"\"\"\n\
          \    import json\n\n    import pandas as pd\n    from sklearn.datasets import\
          \ load_iris\n    from sklearn.model_selection import train_test_split\n\n\
          \    print(\"Setting up drift monitoring...\")\n\n    # Load data for baseline\n\
          \    iris = load_iris()\n    X = pd.DataFrame(iris.data, columns=iris.feature_names)\n\
          \    y = pd.Series(iris.target)\n\n    # Use training data as baseline\n\
          \    X_train, _, y_train, _ = train_test_split(\n        X, y, test_size=0.2,\
          \ random_state=42, stratify=y\n    )\n\n    # Create monitoring configuration\n\
          \    config = {\n        \"baseline_size\": len(X_train),\n        \"features\"\
          : list(iris.feature_names),\n        \"drift_threshold\": 0.5,\n       \
          \ \"monitoring_frequency\": \"daily\",\n        \"alert_channels\": [\"\
          email\", \"slack\"],\n    }\n\n    # Save configuration\n    with open(monitoring_config.path,\
          \ \"w\") as f:\n        json.dump(config, f, indent=2)\n\n    print(\"Drift\
          \ monitoring configured\")\n\n"
        image: python:3.11-slim
    exec-train-iris-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_iris_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==1.5.2'\
          \ 'pandas==2.2.3' 'numpy==1.26.4'  &&  python3 -m pip install --quiet --no-warn-script-location\
          \ 'kfp==2.14.1' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"\
          3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_iris_model(\n    n_estimators: int,\n    test_size: float,\n\
          \    random_state: int,\n    model: Output[Model],\n    scaler: Output[Artifact],\n\
          \    metrics: Output[Metrics],\n) -> str:\n    \"\"\"\n    Train Iris classification\
          \ model.\n\n    This component:\n    - Loads the Iris dataset\n    - Splits\
          \ data into train/test\n    - Trains a Random Forest model\n    - Saves\
          \ model artifacts\n    - Records metrics\n    \"\"\"\n    import json\n\
          \    import pickle\n    from pathlib import Path\n\n    import pandas as\
          \ pd\n    from sklearn.datasets import load_iris\n    from sklearn.ensemble\
          \ import RandomForestClassifier\n    from sklearn.model_selection import\
          \ train_test_split\n    from sklearn.preprocessing import StandardScaler\n\
          \n    print(f\"Training model with {n_estimators} estimators\")\n\n    #\
          \ Load data\n    iris = load_iris()\n    X = pd.DataFrame(iris.data, columns=iris.feature_names)\n\
          \    y = pd.Series(iris.target)\n\n    # Split data\n    X_train, X_test,\
          \ y_train, y_test = train_test_split(\n        X, y, test_size=test_size,\
          \ random_state=random_state, stratify=y\n    )\n\n    # Scale features\n\
          \    scaler_obj = StandardScaler()\n    X_train_scaled = scaler_obj.fit_transform(X_train)\n\
          \    X_test_scaled = scaler_obj.transform(X_test)\n\n    # Train model\n\
          \    rf_model = RandomForestClassifier(\n        n_estimators=n_estimators,\
          \ random_state=random_state\n    )\n    rf_model.fit(X_train_scaled, y_train)\n\
          \n    # Calculate metrics\n    train_score = rf_model.score(X_train_scaled,\
          \ y_train)\n    test_score = rf_model.score(X_test_scaled, y_test)\n\n \
          \   # Save model\n    Path(model.path).parent.mkdir(parents=True, exist_ok=True)\n\
          \    with open(model.path, \"wb\") as f:\n        pickle.dump(rf_model,\
          \ f)\n\n    # Save scaler\n    Path(scaler.path).parent.mkdir(parents=True,\
          \ exist_ok=True)\n    with open(scaler.path, \"wb\") as f:\n        pickle.dump(scaler_obj,\
          \ f)\n\n    # Log metrics\n    metrics.log_metric(\"train_accuracy\", train_score)\n\
          \    metrics.log_metric(\"test_accuracy\", test_score)\n    metrics.log_metric(\"\
          n_features\", X.shape[1])\n    metrics.log_metric(\"n_training_samples\"\
          , len(X_train))\n\n    # Save model directory info\n    model_dir = \"/tmp/model_artifacts\"\
          \n    Path(model_dir).mkdir(parents=True, exist_ok=True)\n    model_info\
          \ = {\n        \"model_type\": \"RandomForestClassifier\",\n        \"n_estimators\"\
          : n_estimators,\n        \"test_accuracy\": test_score,\n        \"train_accuracy\"\
          : train_score,\n    }\n    with open(f\"{model_dir}/model_info.json\", \"\
          w\") as f:\n        json.dump(model_info, f)\n\n    print(f\"Model trained\
          \ - Test accuracy: {test_score:.4f}\")\n\n    # Return the model directory\
          \ path\n    return model_dir\n\n"
        image: python:3.11-slim
pipelineInfo:
  description: End-to-end ML pipeline with training, evaluation, serving, and monitoring
  name: iris-classification-pipeline
root:
  dag:
    tasks:
      condition-1:
        componentRef:
          name: comp-condition-1
        dependentTasks:
        - evaluate-model
        - train-iris-model
        inputs:
          artifacts:
            pipelinechannel--evaluate-model-evaluation_report:
              taskOutputArtifact:
                outputArtifactKey: evaluation_report
                producerTask: evaluate-model
            pipelinechannel--train-iris-model-model:
              taskOutputArtifact:
                outputArtifactKey: model
                producerTask: train-iris-model
            pipelinechannel--train-iris-model-scaler:
              taskOutputArtifact:
                outputArtifactKey: scaler
                producerTask: train-iris-model
          parameters:
            pipelinechannel--evaluate-model-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: evaluate-model
            pipelinechannel--model_name:
              componentInputParameter: model_name
            pipelinechannel--model_version:
              componentInputParameter: model_version
            pipelinechannel--train-iris-model-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: train-iris-model
        taskInfo:
          name: deployment-gate
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--evaluate-model-Output']
            == 'deploy'
      evaluate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model
        dependentTasks:
        - train-iris-model
        inputs:
          artifacts:
            model:
              taskOutputArtifact:
                outputArtifactKey: model
                producerTask: train-iris-model
            scaler:
              taskOutputArtifact:
                outputArtifactKey: scaler
                producerTask: train-iris-model
          parameters:
            accuracy_threshold:
              componentInputParameter: accuracy_threshold
            f1_threshold:
              componentInputParameter: f1_threshold
        taskInfo:
          name: Evaluate Model Performance
      train-iris-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-iris-model
        inputs:
          parameters:
            n_estimators:
              componentInputParameter: n_estimators
            random_state:
              componentInputParameter: random_state
            test_size:
              componentInputParameter: test_size
        taskInfo:
          name: Train Iris Model
  inputDefinitions:
    parameters:
      accuracy_threshold:
        defaultValue: 0.85
        isOptional: true
        parameterType: NUMBER_DOUBLE
      f1_threshold:
        defaultValue: 0.85
        isOptional: true
        parameterType: NUMBER_DOUBLE
      model_name:
        defaultValue: iris-classifier
        isOptional: true
        parameterType: STRING
      model_version:
        defaultValue: v1.0.0
        isOptional: true
        parameterType: STRING
      n_estimators:
        defaultValue: 100.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      random_state:
        defaultValue: 42.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      test_size:
        defaultValue: 0.2
        isOptional: true
        parameterType: NUMBER_DOUBLE
schemaVersion: 2.1.0
sdkVersion: kfp-2.14.1
